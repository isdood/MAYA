@pattern_meta@
GLIMMER Pattern:
{
  "metadata": {
    "timestamp": "2025-06-18 17:24:34",
    "author": "isdood",
    "pattern_version": "1.0.0",
    "color": "#FF69B4"
  },
  "file_info": {
    "path": "./src/knowledge_graph/src/storage/cached_store.rs.orig",
    "type": "orig",
    "hash": "974f3503eebcdb108fc293837eaf65ec702ca717"
  }
}
@pattern_meta@

//! Cached storage implementation for the knowledge graph

use std::sync::Arc;
use std::collections::{HashMap, HashSet, VecDeque};
use std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};
use parking_lot::RwLock;
use rayon::prelude::*;
use crate::storage::batch_optimizer::{BatchConfig, BatchStats};
use std::fmt;
use bincode;
use rayon::prelude::*;
use serde::de::DeserializeOwned;
use serde::Serialize;
use bincode;
use lru::LruCache;
use parking_lot::RwLock;
use crate::error::Result;
use crate::storage::{Storage, WriteBatch, WriteBatchExt, serialize, deserialize};
use crate::error::KnowledgeGraphError;

// Helper error type for CachedStore
#[derive(Debug)]
struct CachedStoreError(String);

impl fmt::Display for CachedStoreError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "CachedStore error: {}", self.0)
    }
}

impl std::error::Error for CachedStoreError {}

impl From<String> for CachedStoreError {
    fn from(err: String) -> Self {
        CachedStoreError(err)
    }
}

impl From<&str> for CachedStoreError {
    fn from(err: &str) -> Self {
        CachedStoreError(err.to_string())
    }
}

/// Performance metrics for the cached store
#[derive(Debug, Default)]
pub struct CacheMetrics {
    hits: AtomicU64,
    misses: AtomicU64,
    read_bytes: AtomicU64,
    write_bytes: AtomicU64,
}

impl Clone for CacheMetrics {
    fn clone(&self) -> Self {
        Self {
            hits: AtomicU64::new(self.hits.load(Ordering::Relaxed)),
            misses: AtomicU64::new(self.misses.load(Ordering::Relaxed)),
            read_bytes: AtomicU64::new(self.read_bytes.load(Ordering::Relaxed)),
            write_bytes: AtomicU64::new(self.write_bytes.load(Ordering::Relaxed)),
        }
    }
}

impl CacheMetrics {
    /// Record a cache hit
    fn record_hit(&self, bytes: usize) {
        self.hits.fetch_add(1, Ordering::Relaxed);
        self.read_bytes.fetch_add(bytes as u64, Ordering::Relaxed);
    }

    /// Record a cache miss
    fn record_miss(&self) {
        self.misses.fetch_add(1, Ordering::Relaxed);
    }

    /// Record bytes written to cache
    fn record_write(&self, bytes: usize) {
        self.write_bytes.fetch_add(bytes as u64, Ordering::Relaxed);
    }

    /// Get cache hit rate
    pub fn hit_rate(&self) -> f64 {
        let hits = self.hits.load(Ordering::Relaxed) as f64;
        let misses = self.misses.load(Ordering::Relaxed) as f64;
        let total = hits + misses;
        if total > 0.0 { hits / total } else { 0.0 }
    }

    /// Get total read bytes
    pub fn read_bytes(&self) -> u64 {
        self.read_bytes.load(Ordering::Relaxed)
    }

    /// Get total write bytes
    pub fn write_bytes(&self) -> u64 {
        self.write_bytes.load(Ordering::Relaxed)
    }
}

/// Configuration for the cached store
#[derive(Debug, Clone)]
pub struct CacheConfig {
    /// Maximum number of items in the cache
    pub capacity: usize,
    /// Enable read-ahead for sequential access patterns
    pub read_ahead: bool,
    /// Number of items to prefetch when read-ahead is enabled
    pub read_ahead_size: usize,
    /// Enable compression for cached values
    pub enable_compression: bool,
}

impl Default for CacheConfig {
    fn default() -> Self {
        Self {
            capacity: 10_000, // Default to 10,000 items
            read_ahead: true,
            read_ahead_size: 100, // Prefetch next 100 items
            enable_compression: true,
        }
    }
}

/// A storage wrapper that adds an LRU cache in front of another storage implementation
pub struct CachedStore<S> {
    inner: S,
    cache: Arc<RwLock<LruCache<Vec<u8>, Vec<u8>>>>,
    metrics: Arc<CacheMetrics>,
    read_ahead_keys: Arc<RwLock<VecDeque<Vec<u8>>>>,
    read_ahead_window: usize,
    config: CacheConfig,
    batch_config: BatchConfig,
}

impl<S> CachedStore<S> {
    /// Create a new cached storage wrapper with default configuration
    pub fn new(inner: S) -> Self {
        let config = CacheConfig::default();
        let batch_config = BatchConfig::default();
        let cache = LruCache::new(config.capacity);
        
        Self {
            inner,
            cache: Arc::new(RwLock::new(cache)),
            metrics: Arc::new(CacheMetrics::default()),
            read_ahead_keys: Arc::new(RwLock::new(VecDeque::with_capacity(1000))),
            read_ahead_window: 10,
            config,
            batch_config,
        }
    }
    
    /// Create a new cached storage wrapper with custom configuration
    pub fn with_config(inner: S, config: CacheConfig, batch_config: BatchConfig) -> Self {
        // Ensure capacity is at least 1 to create a valid NonZeroUsize
        let capacity = std::num::NonZeroUsize::new(config.capacity.max(1)).unwrap();
        let cache = Arc::new(RwLock::new(LruCache::new(capacity)));
        let metrics = Arc::new(CacheMetrics::default());
        
        Self {
            inner,
            cache: cache.clone(),
            metrics: metrics.clone(),
            read_ahead_keys: Arc::new(RwLock::new(VecDeque::with_capacity(config.read_ahead_window * 2))),
            read_ahead_window: config.read_ahead_window,
            config,
            batch_config,
        }
    }
    
    /// Get a reference to the inner storage
    pub fn inner(&self) -> &S {
        &self.inner
    }
    
    /// Get a mutable reference to the inner storage
    pub fn inner_mut(&mut self) -> &mut S {
        &mut self.inner
    }
    
    /// Get the current batch configuration
    pub fn batch_config(&self) -> &BatchConfig {
        &self.batch_config
    }
    
    /// Update the batch configuration
    pub fn set_batch_config(&mut self, config: BatchConfig) {
        self.batch_config = config;
    }
    
    /// Invalidate the cache for a specific key
    pub fn invalidate(&self, key: &[u8]) {
        let mut cache = self.cache.write();
        cache.pop(&key.to_vec());
    }
    
    /// Clear the entire cache
    pub fn clear_cache(&self) {
        let mut cache = self.cache.write();
        cache.clear();
    }

    /// Get cache metrics
    pub fn metrics(&self) -> &CacheMetrics {
        &self.metrics
    }

    /// Update read-ahead keys
    fn update_read_ahead_keys(&self, key: &[u8]) {
        if !self.config.read_ahead {
            return;
        }

        let mut last_keys = self.last_keys.write();
        
        // Add the current key to the history
        last_keys.push_back(key.to_vec());
        
        // Keep only the last N keys
        while last_keys.len() > 10 {
            last_keys.pop_front();
        }
    }

    /// Prefetch keys that are likely to be accessed next
    fn prefetch_keys(&self, _prefix: &[u8]) {
        if !self.config.read_ahead {
            return;
        }

        // In a real implementation, we would analyze the access pattern
        // and prefetch keys that are likely to be accessed next.
        // This is a simplified version that just prefetches sequential keys.
        
        // Example: If the key is "key123", prefetch "key124", "key125", etc.
        // This is just a placeholder - you'd want to implement a more sophisticated
        // prefetching strategy based on your access patterns.
    }
}

impl<S> Storage for CachedStore<S>
where
    S: Storage + 'static,
    S::Batch<'static>: Clone + 'static,
{
    type Batch<'a> = CachedBatch<S::Batch<'a>> where Self: 'a;
    
    fn get<T: DeserializeOwned>(&self, key: &[u8]) -> Result<Option<T>> {
        // Check cache first
        if let Some(cached_bytes) = self.cache.read().unwrap().get(&key.to_vec()) {
            // Deserialize the cached bytes into the target type
            let value: T = bincode::deserialize(cached_bytes)?;
            self.metrics.record_hit(cached_bytes.len());
            return Ok(Some(value));
        }
        
        // Cache miss, get from storage
        self.metrics.record_miss();
        
        // Get the value from the inner storage
        if let Some(value) = self.inner.get(key)? {
            // Serialize the value for caching
            let bytes = bincode::serialize(&value)?;
            // Cache the serialized bytes for future use
            self.cache.write().unwrap().put(key.to_vec(), bytes);
            Ok(Some(value))
        } else {
            Ok(None)
        }
    }

    fn put<T: Serialize>(&self, key: &[u8], value: &T) -> Result<()> {
        // Serialize the value
        let bytes = bincode::serialize(value)?;
        
        // Update the cache with the serialized bytes
        {
            let mut cache = self.cache.write().unwrap();
            cache.put(key.to_vec(), bytes.clone());
            self.metrics.record_write(bytes.len());
        }
        
        // Write to the underlying storage
        self.inner.put_serialized(key, &bytes)
    }

    fn delete(&self, key: &[u8]) -> Result<()> {
        // Invalidate the cache
        {
            let mut cache = self.cache.write().unwrap();
            cache.pop(&key.to_vec());
        }
        
        // Delete from the underlying storage
        self.inner.delete(key)
    }

    fn exists(&self, key: &[u8]) -> Result<bool> {
        // Check cache first
        {
            let cache = self.cache.read().unwrap();
            if cache.contains(&key.to_vec()) {
                return Ok(true);
            }
        }
        
        // Check storage if not in cache
        self.inner.exists(key)
    }
    
    fn get_raw(&self, key: &[u8]) -> Result<Option<Vec<u8>>> {
        // Try to get from cache first
        if let Some(cached_bytes) = self.cache.read().unwrap().get(&key.to_vec()) {
            self.metrics.record_hit(cached_bytes.len());
            return Ok(Some(cached_bytes.clone()));
        }
        
        // Cache miss, get from storage
        self.metrics.record_miss();
        self.inner.get_raw(key)
    }
    
    fn iter_prefix<'a>(&'a self, prefix: &[u8]) -> Box<dyn Iterator<Item = (Vec<u8>, Vec<u8>)> + 'a> {
        self.inner.iter_prefix(prefix)
    }
    
    fn create_batch(&self) -> Self::Batch<'_> {
        CachedBatch {
            inner: self.inner.create_batch(),
            cache: self.cache.clone(),
            metrics: self.metrics.clone(),
            pending_puts: HashMap::new(),
            pending_deletes: HashSet::new(),
            max_batch_size: 10_000,
            total_ops: AtomicUsize::new(0),
        }
    }
}

#[derive(Debug)]
/// A batch of operations that will be applied atomically to the storage
/// and updates the cache accordingly.
///
/// This struct wraps an inner batch and a cache, ensuring that cache updates
/// happen atomically with batch operations. It implements the `WriteBatch`
/// trait to provide a consistent interface with other batch types.
///
/// # Type Parameters
/// - `B`: The inner batch type that implements `WriteBatch`
pub struct CachedBatch<B> {
    inner: B,
    cache: Arc<RwLock<LruCache<Vec<u8>, Vec<u8>>>>>,
    metrics: Arc<CacheMetrics>,
    pending_puts: HashMap<Vec<u8>, Vec<u8>>,
    pending_deletes: HashSet<Vec<u8>>,
    max_batch_size: usize,
    total_ops: AtomicUsize,
    config: BatchConfig,
    stats: Arc<RwLock<BatchStats>>,
}

impl<B> Clone for CachedBatch<B>
where
    B: Clone,
{
    fn clone(&self) -> Self {
        Self {
            inner: self.inner.clone(),
            cache: self.cache.clone(),
            metrics: self.metrics.clone(),
            pending_puts: self.pending_puts.clone(),
            pending_deletes: self.pending_deletes.clone(),
            max_batch_size: self.max_batch_size,
            total_ops: AtomicUsize::new(0),
            config: self.config.clone(),
            stats: self.stats.clone(),
        }
    }
}

impl<B> WriteBatch for CachedBatch<B>
where
    B: WriteBatch + Clone + 'static,
    B: std::any::Any + Send + Sync,
{
    fn put<T: Serialize>(&mut self, key: &[u8], value: &T) -> Result<()> {
        let bytes = bincode::serialize(value).map_err(KnowledgeGraphError::from)?;
        self.put_serialized(key, &bytes)
    }
    
    fn put_serialized(&mut self, key: &[u8], value: &[u8]) -> Result<()> {
        // Check if we need to flush pending operations
        if self.pending_puts.len() + self.pending_deletes.len() >= self.max_batch_size {
            self.apply_pending_ops()?;
        }
        
        // Forward to inner batch
        self.inner.put_serialized(key, value)?;
        
        // Update pending operations
        self.pending_puts.insert(key.to_vec(), value.to_vec());
        self.pending_deletes.remove(key);
        
        // Increment operation count
        self.total_ops.fetch_add(1, Ordering::Relaxed);
        
        Ok(())
    }
    
    fn delete(&mut self, key: &[u8]) -> Result<()> {
        self.delete_serialized(key)
    }
    
    fn delete_serialized(&mut self, key: &[u8]) -> Result<()> {
        // Check if we need to flush pending operations
        if self.pending_puts.len() + self.pending_deletes.len() >= self.max_batch_size {
            self.apply_pending_ops()?;
        }
        
        // Forward to inner batch
        self.inner.delete_serialized(key)?;
        
        // Update pending operations
        self.pending_puts.remove(key);
        self.pending_deletes.insert(key.to_vec());
        
        // Increment operation count
        self.total_ops.fetch_add(1, Ordering::Relaxed);
        
        Ok(())
    }
    
    fn clear(&mut self) {
        self.pending_puts.clear();
        self.pending_deletes.clear();
        self.inner.clear();
    }
    
    fn as_any(&self) -> &dyn std::any::Any {
        self
    }
    
    fn as_mut_any(&mut self) -> &mut dyn std::any::Any {
        self
    }
    
    fn commit(mut self) -> Result<()> {
        // Apply any remaining pending operations
        if let Err(e) = self.apply_pending_ops() {
            // Clear pending operations on error to prevent partial updates
            self.pending_puts.clear();
            self.pending_deletes.clear();
            return Err(e);
        }
        
        // Commit the inner batch
        if let Err(e) = self.inner.commit() {
            // Clear pending operations on error to prevent partial updates
            self.pending_puts.clear();
            self.pending_deletes.clear();
            return Err(e);
        }
        
        // Update the cache with all operations
        let mut cache = match self.cache.write() {
            Ok(cache) => cache,
            Err(poisoned) => {
                // If the lock is poisoned, log the error and continue with the poisoned lock
                log::error!("Cache lock was poisoned, attempting to recover");
                poisoned.into_inner()
            }
        };
        
        // Process puts and deletes in parallel when there are many
        let total_ops = self.total_ops.load(Ordering::Relaxed);
        let enable_parallel = self.config.enable_parallel;
        
        // Always use batch processing for cache updates
        let puts = std::mem::take(&mut self.pending_puts);
        let deletes = std::mem::take(&mut self.pending_deletes);
        
        if enable_parallel && total_ops > 1000 {
            // Process puts and deletes in parallel
            let puts: Vec<_> = puts.into_par_iter().collect();
            let deletes: Vec<_> = deletes.into_par_iter().collect();
            
            // Apply all puts to cache in parallel
            puts.into_par_iter().for_each(|(key, value)| {
                cache.put(key, value);
            });
            
            // Apply all deletes to cache in parallel
            deletes.into_par_iter().for_each(|key| {
                cache.pop(&key);
            });
        } else {
            // Process puts and deletes sequentially
            for (key, value) in puts {
                cache.put(key, value);
            }
            
            for key in deletes {
                cache.pop(&key);
            }
        }
        
        // Update metrics
        let total_bytes: usize = self.pending_puts.values().map(|v| v.len()).sum();
        if total_bytes > 0 {
            self.metrics.record_write(total_bytes);
        }
        
        // Clear pending operations to free memory (should be empty already)
        self.pending_puts.clear();
        self.pending_deletes.clear();
        
        Ok(())
    }
}

impl<B: WriteBatch> CachedBatch<B> {
    /// Create a new CachedBatch with the given configuration
    pub(crate) fn with_config(
        inner: B,
        cache: Arc<RwLock<LruCache<Vec<u8>, Vec<u8>>>>>,
        metrics: Arc<CacheMetrics>,
        config: BatchConfig,
    ) -> Self {
        let stats = BatchStats::new(config.initial_batch_size);
        
        Self {
            inner,
            cache,
            metrics,
            pending_puts: HashMap::new(),
            pending_deletes: HashSet::new(),
            max_batch_size: config.initial_batch_size,
            total_ops: AtomicUsize::new(0),
            config,
            stats: Arc::new(RwLock::new(stats)),
        }
    }
    
    /// Create a new CachedBatch with default configuration
    pub(crate) fn new(
        inner: B,
        cache: Arc<RwLock<LruCache<Vec<u8>, Vec<u8>>>>>,
        metrics: Arc<CacheMetrics>,
    ) -> Self {
        let config = BatchConfig::default();
        Self::with_config(inner, cache, metrics, config)
    }
    /// Apply pending operations to the inner batch and clear them
    fn apply_pending_ops(&mut self) -> Result<()> {
        if self.pending_puts.is_empty() && self.pending_deletes.is_empty() {
            return Ok(());
        }
        
        let start_time = std::time::Instant::now();
        let batch_size = self.stats.read().batch_size();
        let enable_parallel = self.config.enable_parallel;
        
        // Process puts in batches
        let puts: Vec<_> = self.pending_puts.drain().collect();
        if !puts.is_empty() {
            if enable_parallel && puts.len() > batch_size {
                // Process puts in parallel chunks
                puts.par_chunks(batch_size)
                    .try_for_each(|chunk| {
                        let mut inner_batch = self.inner.create_batch();
                        for (k, v) in chunk {
                            inner_batch.put_serialized(k, v)?;
                        }
                        inner_batch.commit()
                    })?;
            } else {
                // Process puts sequentially in chunks
                for chunk in puts.chunks(batch_size) {
                    let mut inner_batch = self.inner.create_batch();
                    for (k, v) in chunk {
                        inner_batch.put_serialized(k, v)?;
                    }
                    inner_batch.commit()?;
                }
            }
        }
        
        // Process deletes in batches
        let deletes: Vec<_> = self.pending_deletes.drain().collect();
        if !deletes.is_empty() {
            if enable_parallel && deletes.len() > batch_size {
                // Process deletes in parallel chunks
                deletes.par_chunks(batch_size)
                    .try_for_each(|chunk| {
                        let mut inner_batch = self.inner.create_batch();
                        for k in chunk {
                            inner_batch.delete_serialized(k)?;
                        }
                        inner_batch.commit()
                    })?;
            } else {
                // Process deletes sequentially in chunks
                for chunk in deletes.chunks(batch_size) {
                    let mut inner_batch = self.inner.create_batch();
                    for k in chunk {
                        inner_batch.delete_serialized(k)?;
                    }
                    inner_batch.commit()?;
                }
            }
        }
        
        // Update statistics
        let duration = start_time.elapsed();
        self.stats.write().record_batch(puts.len() + deletes.len(), duration);
        
        Ok(())
    }
}

impl<S> WriteBatchExt for CachedStore<S>
where
    S: Storage + WriteBatchExt + 'static,
    S::Batch<'static>: Clone + 'static,
{
    type Batch<'a> = CachedBatch<S::Batch<'a>> where Self: 'a;
    
    fn batch(&self) -> Self::Batch<'_> {
        self.create_batch()
    }
    
    fn create_batch(&self) -> Self::Batch<'_> {
        CachedBatch {
            inner: self.inner.create_batch(),
            cache: self.cache.clone(),
            metrics: self.metrics.clone(),
            pending_puts: HashMap::new(),
            pending_deletes: HashSet::new(),
            max_batch_size: 10_000,
            total_ops: AtomicUsize::new(0),
        }
    }
    
    fn put_serialized<T: Serialize>(&self, key: &[u8], value: &T) -> Result<()> {
        // Serialize the value
        let bytes = bincode::serialize(value)?;
        
        // Update the cache
        {
            let mut cache = self.cache.write().unwrap();
            cache.put(key.to_vec(), bytes.clone());
            self.metrics.record_write(bytes.len());
        }
        
        // Write to the underlying storage
        self.inner.put_serialized(key, &bytes)
    }
    
    fn delete_serialized(&self, key: &[u8]) -> Result<()> {
        // Invalidate the cache
        {
            let mut cache = self.cache.write().unwrap();
            cache.pop(&key.to_vec());
        }
        
        // Delete from the underlying storage
        self.inner.delete_serialized(key)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::storage::sled_store::SledStore;
    use tempfile::tempdir;
    
    #[derive(Debug, Clone, PartialEq, serde::Serialize, serde::Deserialize)]
    struct TestValue {
        data: String,
        count: u64,
    }
    
    #[test]
    fn test_cached_store() -> Result<()> {
        // Create a unique temporary directory for the first SledStore
        let temp_dir1 = tempfile::tempdir()?;
        let test_path1 = temp_dir1.path().to_path_buf();
        
        // Create a new CachedStore with default config
        let inner = SledStore::open(&test_path1)?;
        let cache = CachedStore::new(inner);
        
        // Test basic operations
        
        // Test put and get
        let key = b"test_key";
        let value = TestValue {
            data: "test data".to_string(),
            count: 42,
        };
        
        // Put should cache the value
        cache.put(key, &value)?;
        
        // Get should hit the cache
        let cached: Option<TestValue> = cache.get(key)?;
        assert_eq!(cached, Some(value.clone()));
        
        // Verify metrics
        assert!(cache.metrics().hit_rate() > 0.0);
        
        // Test delete
        cache.delete(key)?;
        let deleted: Option<TestValue> = cache.get(key)?;
        assert_eq!(deleted, None);
        
        // Test batch operations with a serializable type
        {
            let mut batch = <CachedStore<_> as Storage>::batch(&cache);
            
            // Create test values that implement Serialize/Deserialize
            let test_value1 = TestValue { data: "batch1".to_string(), count: 1 };
            let test_value2 = TestValue { data: "batch2".to_string(), count: 2 };
            
            // Put serialized values using the module's serialize function
            let serialized1 = super::super::serialize(&test_value1)?;
            let serialized2 = super::super::serialize(&test_value2)?;
            batch.put_serialized(b"batch_key1", &serialized1)?;
            batch.put_serialized(b"batch_key2", &serialized2)?;
            batch.delete(b"batch_key1")?;
            batch.commit()?;
        }
        
        // Verify batch operations
        assert_eq!(cache.get::<TestValue>(b"batch_key1")?, None);
        let batch_val: Option<TestValue> = cache.get(b"batch_key2")?;
        assert_eq!(batch_val, Some(TestValue { data: "batch2".to_string(), count: 2 }));
        
        // Test cache invalidation
        cache.invalidate(b"key2");
        
        // Test clear cache
        cache.clear_cache();
        
        // Test with custom configuration using a new temporary directory
        let temp_dir2 = tempfile::tempdir()?;
        let test_path2 = temp_dir2.path().to_path_buf();
        
        let inner = SledStore::open(&test_path2)?;
        let config = CacheConfig {
            capacity: 100,
            read_ahead: true,
            read_ahead_size: 10,
            enable_compression: true,
        };
        let _cache = CachedStore::with_config(inner, config);
        
        Ok(())
    }
    
    #[test]
    fn test_metrics() -> Result<()> {
        let temp_dir = tempdir()?;
        let inner = SledStore::open(temp_dir.path())?;
        let cache = CachedStore::new(inner);
        
        // Initial metrics should be zero
        assert_eq!(cache.metrics().hit_rate(), 0.0);
        assert_eq!(cache.metrics().read_bytes(), 0);
        assert_eq!(cache.metrics().write_bytes(), 0);
        
        // Add some data
        cache.put(b"key1", &"value1".to_string())?;
        
        // Read should be a miss first, then hit
        let _: Option<String> = cache.get(b"key1")?; // Miss
        let _: Option<String> = cache.get(b"key1")?; // Hit
        
        // Check metrics
        assert!(cache.metrics().hit_rate() > 0.0);
        assert!(cache.metrics().read_bytes() > 0);
        assert!(cache.metrics().write_bytes() > 0);
        
        Ok(())
    }
}
