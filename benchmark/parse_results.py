@pattern_meta@
GLIMMER Pattern:
{
  "metadata": {
    "timestamp": "2025-06-18 12:41:05",
    "author": "isdood",
    "pattern_version": "1.0.0",
    "color": "#FF69B4"
  },
  "file_info": {
    "path": "./benchmark/parse_results.py",
    "type": "py",
    "hash": "5119b651950e28099fc4136d6b23119e000a617a"
  }
}
@pattern_meta@

#!/usr/bin/env python3
"""
Script to parse Criterion benchmark results and update the RESULTS.md file.
"""
import json
import os
import re
from datetime import datetime
from pathlib import Path

# Paths
BENCHMARK_DIR = Path("target/criterion")
RESULTS_FILE = Path("benchmark/RESULTS.md")

def parse_benchmark_results():
    """Parse Criterion benchmark results and return structured data."""
    results = {}
    
    # Find all benchmark directories
    for bench_dir in BENCHMARK_DIR.glob("**/new"):
        benchmark_name = bench_dir.parent.name
        
        # Read the estimate file
        estimate_file = bench_dir / "estimates.json"
        if not estimate_file.exists():
            continue
            
        with open(estimate_file) as f:
            estimates = json.load(f)
            
        # Extract relevant metrics
        results[benchmark_name] = {
            "mean": estimates["mean"],
            "std_dev": estimates.get("std_dev", {}),
            "median": estimates.get("median", {}),
            "median_abs_dev": estimates.get("median_abs_dev", {}),
            "slope": estimates.get("slope", {}),
            "r2": estimates.get("r2", {})
        }
    
    return results

def update_results_md(results):
    """Update the RESULTS.md file with benchmark results."""
    if not RESULTS_FILE.exists():
        print(f"Error: {RESULTS_FILE} not found")
        return
    
    # Read current content
    with open(RESULTS_FILE, 'r') as f:
        content = f.read()
    
    # Update results section
    for bench_name, metrics in results.items():
        section = f"### {bench_name}\n"
        
        # Add mean time
        mean = metrics["mean"]
        section += f"- **Mean time**: {mean['point_estimate']/1e6:.2f} ms/op (Â±{mean['standard_error']/1e6:.2f} ms/op)\n"
        
        # Add throughput if available
        if "throughput" in metrics["mean"]:
            tp = metrics["mean"]["throughput"]
            section += f"- **Throughput**: {tp['value']/1e6:.2f} MB/s\n"
        
        # Update or add the section
        pattern = rf"(### {re.escape(bench_name)}[\s\S]*?)(?=### |\Z)"
        new_section = section + "\n"
        
        if re.search(pattern, content):
            content = re.sub(pattern, new_section, content)
        else:
            # If section doesn't exist, add it before the "## Detailed Results" section
            content = content.replace("## Detailed Results", f"{new_section}\n## Detailed Results")
    
    # Add timestamp
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    content = re.sub(
        r"Generated by MAYA Benchmark Suite",
        f"Last updated: {timestamp}\n\nGenerated by MAYA Benchmark Suite",
        content
    )
    
    # Write back to file
    with open(RESULTS_FILE, 'w') as f:
        f.write(content)

def main():
    """Main function to parse and update benchmark results."""
    if not BENCHMARK_DIR.exists():
        print(f"Error: Benchmark directory not found at {BENCHMARK_DIR}")
        print("Please run 'cargo bench' first to generate benchmark results.")
        return
    
    try:
        results = parse_benchmark_results()
        if not results:
            print("No benchmark results found.")
            return
            
        update_results_md(results)
        print(f"Successfully updated {RESULTS_FILE}")
    except Exception as e:
        print(f"Error processing benchmark results: {e}")

if __name__ == "__main__":
    main()
